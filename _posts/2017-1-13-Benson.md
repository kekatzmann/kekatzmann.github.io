---
layout: post
title: Benson
---

What's up guys. Week one at Metis is officially in the books. Or rather, in github. This week we were assigned to employ our data science skills on a project for WomenTechWomenYes, a fictional non-profit organization aimed to increase the presence of women in technology. They are hosting a gala in early summer to increase awareness and reach of their organization, and are planning to deploy street teams at subway entrances to fill event space. Our task was simple: to optimize the placement of their street teams in order to maximize awareness and contributions. In order to do so, we initially used MTA turnstile data, available for free at MTA.info. Later, we included data on average incomes around each subway entrance, as a proxy for generosity and contribution magnitude.

When we first downloaded the MTA turnstile data and checked it out with pandas, we noticed it was very messy. It needed much cleaning. The data was cumulative for each turnstile unit, for each timestamp. Each time  was roughly 4 hours of turnstile turn data. Most were taken at 12am, 4am, 8am, 12p, 4pm, and 8pm; But this was not consistent. Also, some of the total counts (after the cumulative amounts were converted into deltas), were negative values. And, outliers were abundant throughout. Not normal outliers though, these data points were indicating multi-billion entries or exits for some turnstiles in a 4-hour period. Safe to say, this is not very probable, so we removed those. In fact, after looking at the data, we removed all data points that read anything over 10,000 entries or exits per turnstile per 4-hour period. Which itself was a very conservative cutoff. Most turnstiles recorded around 1,500 entries or exits. Some recorded up to 4,000. 10,000 still included outliers, but the noise was tremendously reduced from the raw data. (side note: we were working on removing outliers greater than 3 standard deviations from the mean, but those values in the billions threw off the mean far too much. Plus, we would need a lot of data for one turnstile under very similar conditions (i.e. time of day, day of week, week of year). In hindsight, using the median could have worked. But the linear cutoff at 10,000 did just fine, frankly.) 

After we cleaned up the data real nice, we were able to determine which stations had the most traffic and at which times. We then imported and merged income data, which we acquired with the help of Vinny's unparalleled hacking skills (Vinny is one of our instructors). After the data was merged, we were able to cross-reference the busiest stations with stations in high-income areas. In the end, we came up with a list of ten suggested stations for the WTWY street teams to target.

